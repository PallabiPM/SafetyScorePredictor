# -*- coding: utf-8 -*-
"""Safety Score Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WULQKN0CL94XdeM-4szmS2A6cuchaKfe

CREATE DUMMY DATA FOR PARKING SAFETY SCORE CALCULATION
"""

import pandas as pd
import numpy as np

np.random.seed(42)  # For reproducibility

# Constants for Germany approximate bounding box
lat_min, lat_max = 47.27, 55.06
lon_min, lon_max = 5.87, 15.04

# Data generation parameters
types = ['On Street', 'Basement', 'Parking Lot', 'Private Garage']
type_weights = [0.4, 0.2, 0.25, 0.15]

times_of_day = ['Morning', 'Afternoon', 'Evening', 'Night']
traffic_levels = ['Low', 'Medium', 'High']
weather_conditions = ['Clear', 'Rain', 'Snow', 'Fog']
lighting_conditions = ['Good', 'Moderate', 'Poor']
cctv_prob = {'On Street': 0.2, 'Basement': 0.6, 'Parking Lot': 0.5, 'Private Garage': 0.8}

# Risk scores for scoring
risk_weather = {'Clear': 0, 'Rain': 5, 'Snow': 10, 'Fog': 7}
risk_time = {'Morning': 1, 'Afternoon': 1, 'Evening': 3, 'Night': 7}
risk_type = {'On Street': 7, 'Basement': 3, 'Parking Lot': 5, 'Private Garage': 1}
risk_lighting = {'Good': 0, 'Moderate': 3, 'Poor': 7}

data = []

for _ in range(300):
    lat = np.random.uniform(lat_min, lat_max)
    lon = np.random.uniform(lon_min, lon_max)

    p_type = np.random.choice(types, p=type_weights)
    occupancy = np.random.beta(2, 5)  # Skewed to less crowded spots but with spread
    time = np.random.choice(times_of_day)
    traffic = np.random.choice(traffic_levels)
    weather = np.random.choice(weather_conditions)

    # Lighting: Higher chance of poor lighting at night
    if time == 'Night':
        lighting = np.random.choice(lighting_conditions, p=[0.2, 0.3, 0.5])
    else:
        lighting = np.random.choice(lighting_conditions, p=[0.6, 0.3, 0.1])

    cctv = np.random.rand() < cctv_prob[p_type]

    # Traffic numeric risk: Low=1, Medium=5, High=10
    traffic_risk = {'Low':1, 'Medium':5, 'High':10}[traffic]

    # Calculate collision history expected rate (lambda) based on risk factors
    lambda_collision = (
        0.5 * risk_type[p_type] +
        0.4 * occupancy * 10 +
        0.3 * traffic_risk +
        0.3 * risk_weather[weather] +
        0.3 * risk_lighting[lighting] +
        0.2 * risk_time[time] -
        (5 if cctv else 0)
    )
    lambda_collision = max(0.1, lambda_collision / 10)  # keep lambda positive and reasonable

    collision_history = np.random.poisson(lam=lambda_collision)

    # Calculate safety score (0-100)
    safety_score = 100 - (
        7 * collision_history +
        5 * occupancy * 10 +
        5 * traffic_risk +
        4 * risk_weather[weather] +
        4 * risk_lighting[lighting] +
        3 * risk_time[time] +
        6 * risk_type[p_type] -
        (8 if cctv else 0)
    )
    safety_score = max(0, min(100, safety_score))  # Clamp between 0 and 100

    data.append([lat, lon, p_type, round(occupancy,2), time, traffic, weather, lighting, cctv, collision_history, round(safety_score,2)])

df = pd.DataFrame(data, columns=[
    'Latitude', 'Longitude', 'Type of Parking Spot', 'Occupancy Density', 'Time of Day', 'Traffic Level',
    'Weather', 'Lighting', 'CCTV Coverage', 'Collision History', 'Safety Score'
])

# Export to CSV
df.to_csv("germany_parking_safety_dataset.csv", index=False)
print("CSV file 'germany_parking_safety_dataset.csv' generated successfully.")

"""TRAIN THE MODEL FOR REGRESSION"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Load the dataset (if you already have it in df, skip this step)
df = pd.read_csv("germany_parking_safety_dataset.csv")

# Features and target
X = df.drop(columns=['Safety Score'])
y = df['Safety Score']

# Convert boolean to int
X['CCTV Coverage'] = X['CCTV Coverage'].astype(int)

# Categorical columns to encode
categorical_cols = ['Type of Parking Spot', 'Time of Day', 'Traffic Level', 'Weather', 'Lighting']

# One-hot encode categorical features
encoder = OneHotEncoder()
X_encoded_sparse = encoder.fit_transform(X[categorical_cols])
X_encoded = X_encoded_sparse.toarray()  # convert sparse matrix to dense


# Combine encoded categorical and numeric columns
numeric_cols = ['Latitude', 'Longitude', 'Occupancy Density', 'CCTV Coverage', 'Collision History']
X_numeric = X[numeric_cols].values

X_final = np.hstack([X_numeric, X_encoded])

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Initialize XGBoost regressor
model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

# Train model
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.2f}")
print(f"R²: {r2:.2f}")

"""TESTING THE DATA"""

import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

# Assuming df is your full dataset loaded and preprocessed (boolean to int)
df['CCTV Coverage'] = df['CCTV Coverage'].astype(int)

# Prepare features using the previously fitted encoder and numeric columns
X_all_cat_sparse = encoder.transform(df[categorical_cols])
X_all_cat = X_all_cat_sparse.toarray()
X_all_num = df[numeric_cols].values
X_all = np.hstack([X_all_num, X_all_cat])

# Predict with the trained model
predicted_scores = model.predict(X_all)

# Add predictions to dataframe
df['Predicted Safety Score'] = predicted_scores

# Display comparison of actual vs predicted safety scores
print(df[['Safety Score', 'Predicted Safety Score']].head(10))

# Evaluate predictions
rmse_all = np.sqrt(mean_squared_error(df['Safety Score'], df['Predicted Safety Score']))
r2_all = r2_score(df['Safety Score'], df['Predicted Safety Score'])

print(f"\nEvaluation on entire dataset:")
print(f"RMSE: {rmse_all:.2f}")
print(f"R²: {r2_all:.2f}")

"""RUN THE CODE WITH PARAMETERS AS INPUT"""

import numpy as np
import pandas as pd

def predict_parking_safety_score(
    latitude: float,
    longitude: float,
    parking_type: str,
    occupancy_density: float,
    time_of_day: str,
    traffic_level: str,
    weather: str,
    lighting: str,
    cctv_coverage: bool,
    collision_history: int,
    encoder,
    model
) -> float:
    """
    Predict safety score for a parking spot using a trained XGBoost model.

    Parameters:
        latitude (float): Latitude coordinate.
        longitude (float): Longitude coordinate.
        parking_type (str): Type of parking spot (e.g., 'On Street').
        occupancy_density (float): Occupancy density (0 to 1).
        time_of_day (str): Time of day ('Morning', 'Afternoon', etc.).
        traffic_level (str): Traffic level ('Low', 'Medium', 'High').
        weather (str): Weather condition ('Clear', 'Rain', etc.).
        lighting (str): Lighting condition ('Good', 'Moderate', 'Poor').
        cctv_coverage (bool): CCTV coverage (True or False).
        collision_history (int): Number of collisions reported in 3 months.
        encoder (OneHotEncoder): Fitted OneHotEncoder from training.
        model (XGBRegressor): Trained XGBoost regression model.

    Returns:
        float: Predicted safety score (0 to 100).
    """

    # Create dataframe for a single row with categorical and numeric data
    input_df_cat = pd.DataFrame([{
        'Type of Parking Spot': parking_type,
        'Time of Day': time_of_day,
        'Traffic Level': traffic_level,
        'Weather': weather,
        'Lighting': lighting
    }])

    # Encode categorical features
    encoded_cat = encoder.transform(input_df_cat).toarray()

    # Numeric features in the same order used during training
    numeric_features = np.array([
        latitude,
        longitude,
        occupancy_density,
        int(cctv_coverage),
        collision_history
    ]).reshape(1, -1)

    # Combine numeric and encoded categorical features
    X_input = np.hstack([numeric_features, encoded_cat])

    # Predict using the model
    predicted_score = model.predict(X_input)[0]

    # Clamp the result between 0 and 100 (optional, if your model can output outside range)
    predicted_score = max(0, min(100, predicted_score))

    return predicted_score

"""RUN A SCENARIO"""

score = predict_parking_safety_score(
    latitude=52.52,
    longitude=13.405,
    parking_type='Private Garage',
    occupancy_density=0.3,
    time_of_day='Evening',
    traffic_level='Low',
    weather='Clear',
    lighting='Good',
    cctv_coverage=True,
    collision_history=2,
    encoder=encoder,
    model=model
)
print(f"Predicted Safety Score: {score:.2f}")

"""SAVE MODELS"""

import joblib

# Save model
joblib.dump(model, 'xgb_model.joblib')

# Save encoder
joblib.dump(encoder, 'encoder.joblib')

"""API Creation to integrate the XGBoost to n8n"""

from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np
import pandas as pd

# Load model and encoder at startup
model = joblib.load('xgb_model.joblib')
encoder = joblib.load('encoder.joblib')

class ParkingSpotInput(BaseModel):
    latitude: float
    longitude: float
    parking_type: str
    occupancy_density: float
    time_of_day: str
    traffic_level: str
    weather: str
    lighting: str
    cctv_coverage: bool
    collision_history: int

app = FastAPI()

def predict_parking_safety_score(data: ParkingSpotInput) -> float:
    input_df_cat = pd.DataFrame([{
        'Type of Parking Spot': data.parking_type,
        'Time of Day': data.time_of_day,
        'Traffic Level': data.traffic_level,
        'Weather': data.weather,
        'Lighting': data.lighting
    }])
    encoded_cat = encoder.transform(input_df_cat).toarray()
    numeric_features = np.array([
        data.latitude,
        data.longitude,
        data.occupancy_density,
        int(data.cctv_coverage),
        data.collision_history
    ]).reshape(1, -1)
    X_input = np.hstack([numeric_features, encoded_cat])
    pred = model.predict(X_input)[0]
    return max(0, min(100, pred))

class ParkingSpotInput(BaseModel):
    latitude: float
    longitude: float
    parking_type: str
    occupancy_density: float
    time_of_day: str
    traffic_level: str
    weather: str
    lighting: str
    cctv_coverage: bool
    collision_history: int

# ✅ Initialize FastAPI app
app = FastAPI()

# ✅ Define prediction function
def predict_parking_safety_score(data: ParkingSpotInput) -> float:
    input_df_cat = pd.DataFrame([{
        'Type of Parking Spot': data.parking_type,
        'Time of Day': data.time_of_day,
        'Traffic Level': data.traffic_level,
        'Weather': data.weather,
        'Lighting': data.lighting
    }])

    encoded_cat = encoder.transform(input_df_cat).toarray()

    numeric_features = np.array([
        data.latitude,
        data.longitude,
        data.occupancy_density,
        int(data.cctv_coverage),
        data.collision_history
    ]).reshape(1, -1)

    X_input = np.hstack([numeric_features, encoded_cat])

    pred = model.predict(X_input)[0]
    return max(0, min(100, pred))  # Clipping between 0 and 100

# ✅ Define API endpoint
@app.post("/predict_safety_score/")
async def predict_safety_score(input_data: ParkingSpotInput):
    score = predict_parking_safety_score(input_data)
    return {"predicted_score": round(float(score), 2)}
